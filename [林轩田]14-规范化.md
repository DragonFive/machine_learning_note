
[TOC]

>监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。

>我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。

>另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。

>要知道，有时候人的先验是非常重要的。前人的经验会让你少走很多弯路，这就是为什么我们平时学习最好找个大牛带带的原因。一句点拨可以为我们拨开眼前乌云，还我们一片晴空万里，醍醐灌顶。对机器学习也是一样，如果被我们人稍微点拨一下，它肯定能更快的学习相应的任务。只是由于人和机器的交流目前还没有那么直接的方法，目前这个媒介只能由规则项来担当了。
——引用自 大神 邹博

# 规范化多项式集合

regularition : 是解决overfitting的一种方法 。

低次方的多项式集合会包在高次方的多项式集合里面 。

## 回归约束

规范化的过程就是，从高次多项式走回到低次多项式。就是在问题里面加上一些条件，约束：高次的系数都为0；


![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-4a91d8c82c7f4f03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

也就是我想找一个二次多项式，实际上它也是个十次多项式，只不过他的3次以上的系数都是0；


## 宽松的回归约束  looser constraint  稀疏规则化
只限制等于0的参数的个数，而不限制究竟哪个参数是0；这个其实是**L0范数**，规则化的结果是使得W**稀疏**
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-d38340d090cb0519.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

当我们看到 式子中出现的布尔表达式时，像PLA一样它的最优化是NP-hard的问题。

## 规则化的Wreg

这里我们只要求这个参数向量W的模要有限制。不关心它到底有几个参数，这样的多项式集合我们叫做Hc

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-fe9d3d732b2bd435.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

H2与Hc是有重合部分的。overlaps。 包含结构也是有的。这样我们就跳脱了NP-hard的困境。
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-34d3cd8a1034ef3d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们把这样的多项式集合得出的参数叫做规则化的w（在我们规则的条件下找出来的w）

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-f4f49a94112be197.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


# weight decay regularization
是L2范数: ||W||2。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多吧，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。过拟合通俗的讲法就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。
那么为什么L2范数能防止过拟合呢。 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别的哦。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。
L2范数可以防止过拟合，提升模型的泛化能力。
## 矩阵形式表示规则化回归问题 


![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-9cd0ddd507ea796a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
上面是我们要优化的带上约束的表达式。对于式子里面每一子项的平方和的形式可以变换成求一个向量的L2范数平方的形式。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-88a5b9db55ff029c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
转化为这个样子 ，这个条件是我们的W需要在半径为根号C的圆里面。
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-6cc8cf0617cbb9c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在优化的过程中，如果没有条件，梯度的反方向就是我们的目标函数要减小的方向。但是我们现在有了约束条件，我们不能走出圆的范围，如果下降的方向有个分量与圆的法向量的方向垂直。如果梯度的反方向与圆的法向量平行，那么我们就不能继续优化了。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-0cf78fafd247cc3a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

所以我们优化的结果是梯度的反方向与圆的法向量平行。
在下面的推导中，我们可以看出求Wreg就是一个线性的运算，这个过程在统计学中叫做岭回归"ridge regression"
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-2cfa86c09cb8badb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
其实我们求上面那个式子等于零，就相当于找它的积分的最小值咯，这样我们的最优化的目标函数其实稍微有了一点改变。我们叫它augmented Error 加上去的错误。
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-be716244fdbc3d33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
民间还有个说法就是，规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。
>一般来说，监督学习可以看做最小化下面的目标函数：
![](http://img.blog.csdn.net/20140504122253546?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvem91eHkwOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)
其中，第一项L(y<sub>i</sub>,f(x<sub>i</sub>;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(x<sub>i</sub>;w)和真实的标签y<sub>i</sub>之前的误差。因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数w的规则化函数Ω(w)去约束我们的模型尽量的简单。
——引用自 大神 邹博



从下面的图片可以看出，加上一点点的λ，效果就会很好了。
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-0a60979e4b603fd8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

把![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-958977650518c9df.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)叫做权值递减规则化，过大的λ会使得对W的限制变得大，W就会去得很小。

当数据都处于[-1,1]之间时，对于高次幂 $$ X^Q_n $$ 的数据，就会比其他次幂小的多。如果我们模型需要高次幂，但是$$ X^Q_n $$很小，那么就必须增大权重来提高其影响力，但是惩罚函数又会限制权重的增大，这就照成了问题。方法就是让Φ(x)内的向量是**互相正交的**，即Legendre　polynomials 勒让德多项式。

![这里写图片描述](http://img.blog.csdn.net/20160812114822402)

q+1次方的多项式进行了一些坐标准换，但是因为各个次数多项式之间不是垂直的，这导致在次数较低的地方效果还不错，但是次数高的地方，惩罚过重。在多项式空间里面找到垂直的多项式。

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-95849d6de1fd16a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

# 选择最好的惩罚函数

1. 根据我们想要的target function ｆ来选 
比如，我知道我的ｆ(x)是偶函数，那么我就希望我的g内偶次幂的权重大。即我要尽可能降低我奇次幂的权重。即把惩罚函数设为
    
  ![这里写图片描述](http://img.blog.csdn.net/20160812115705008)
2.  如果我希望我们模型光滑，简单，那就用Ｌ１规范 
   ![这里写图片描述](http://img.blog.csdn.net/20160812120153810)
3.  如果我希望我的模型任意达到最优，就是效果好，那就用Ｌ２规范 
   ![这里写图片描述](http://img.blog.csdn.net/20160812115909870) 
   L1要求低，精度低，但计算量小

## L1,L2 规范

![这里写图片描述](http://img.blog.csdn.net/20160812120249559) 
L1要求低，精度低，但计算量小，是凸函数的，但w=0时是不可微分的。它的解常常是稀疏的。
L2规范化比较平滑是凸函数。

从下图我们可以看出，不同的noise需要不同的λ。
![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-1040eaacaa306842.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们选择规则项（惩罚项）应该看它惩罚谁比较重，就知道它倾向于选择什么样的hypothesis.

![Paste_Image.png](http://upload-images.jianshu.io/upload_images/454341-a1b2f0b31a810ddc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)







# 参考资料

[[机器学习中的范数规则化之（一）L0、L1与L2范数 ](http://blog.csdn.net/zouxy09/article/details/24971995)

[regularization 规范化（L1，L2等等）：加惩罚函数降低过拟合](http://blog.csdn.net/mosbest/article/details/52188945)






